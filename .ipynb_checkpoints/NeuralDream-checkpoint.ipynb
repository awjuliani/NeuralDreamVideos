{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating NeuralDream Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import cPickle\n",
    "\n",
    "from model_rnn import Model\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import os\n",
    "from model_vae import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up project name and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project_name = 'video'\n",
    "\n",
    "input_path = './inputs/'+project_name\n",
    "output_path = './outputs/'+project_name\n",
    "vae_checkpoints = './vae_cp/'+project_name\n",
    "lstm_checkpoints = './lstm_cp/'+project_name\n",
    "paths = [input_path,output_path,vae_checkpoints,lstm_checkpoints]\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing source video\n",
    "\n",
    "Resize the desired video to 128x128 using ffmpeg from the terminal or your preferred video processing software.\n",
    "\n",
    "`ffmpeg -i input.mp4 -vf scale=128:128 output.mp4`\n",
    "\n",
    "Next we use the smaller video to generate ten frames per second, and save them all to a frame folder.\n",
    "\n",
    "`ffmpeg -i output.mp4 -r 10 -f image2 ./inputs/project_name/%05d.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=256, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=256, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=256, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=256, # 2nd layer decoder neurons\n",
    "         n_input=49152, # Number of values per video frame\n",
    "         n_z=64)  # dimensionality of latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the frames into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadImages(data):\n",
    "    images = []\n",
    "    for myFile in data:\n",
    "        img = Image.open(myFile)\n",
    "        #img = img.resize((128,128))\n",
    "        images.append(np.reshape(np.array(img),[49152]))\n",
    "    images = np.array(images)\n",
    "    images = images.astype('float32')\n",
    "    images = images / 256\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataS = sorted(glob(os.path.join(\"./inputs/\", project_name, \"*.png\")))\n",
    "imagesS = loadImages(dataS)\n",
    "n_samples = len(imagesS)\n",
    "print 'Frames loaded. There are ',str(n_samples),'frames in project',project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_vae(network_architecture, learning_rate=1e-4,\n",
    "          batch_size=50, training_epochs=10, display_step=5,model_path='./vae_checkpoints'):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size,load_model = False,checkpoint_folder=model_path)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples / batch_size)\n",
    "        # Loop over all batches\n",
    "        perms = np.random.permutation(imagesS)\n",
    "        for i in range(perms.shape[0]/batch_size):\n",
    "            batch_xs = perms[i *batch_size:(i+1) * batch_size,:]\n",
    "\n",
    "            # Fit training using batch data\n",
    "            cost = vae.partial_fit(batch_xs)\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples * batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \\\n",
    "                  \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "    vae.save_model(epoch,model_path)\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 50\n",
    "vae = train_vae(network_architecture, training_epochs=100,model_path='./vae_cp/'+project_name,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the reconstruction capacity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_sample = imagesS[0:batch_size]\n",
    "x_reconstruct = vae.reconstruct(x_sample)\n",
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(5):\n",
    "\n",
    "    plt.subplot(5, 2, 2*i + 1)\n",
    "    plt.imshow((x_sample[i+0].reshape(128, 128,3)), vmin=0, vmax=1)\n",
    "    plt.title(\"Test input\")\n",
    "    plt.subplot(5, 2, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[i+0].reshape(128, 128,3), vmin=0, vmax=1)\n",
    "    plt.title(\"Reconstruction\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate latent representation of each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_z = vae.transform(imagesS)\n",
    "with open('./data/'+project_name+'_Z.p','w') as f:\n",
    "    cPickle.dump(x_z,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model and loading latent representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class args():\n",
    "    def __init__(self):\n",
    "        self.rnn_size = 256\n",
    "        self.num_layers = 2\n",
    "        self.model = 'lstm'\n",
    "        self.batch_size = 5\n",
    "        self.seq_length = 50\n",
    "        self.num_epochs = 150\n",
    "        self.save_every = 1000\n",
    "        self.grad_clip = 5.\n",
    "        self.learning_rate = 2e-2\n",
    "        self.decay_rate = 0.97\n",
    "        self.input_size = 64\n",
    "        self.save_dir = './lstm_cp/'+project_name\n",
    "        \n",
    "argsA = args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/'+project_name+'_Z.p','r') as f:\n",
    "    z_images = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batches():\n",
    "    num_batches = len(z_images) / (argsA.batch_size * argsA.seq_length)\n",
    "    tensor = z_images[:num_batches * argsA.batch_size * argsA.seq_length]\n",
    "    xdata = tensor\n",
    "    ydata = np.copy(tensor)\n",
    "    ydata[:-1] = xdata[1:]\n",
    "    ydata[-1] = xdata[0]\n",
    "    x_batches = np.split(xdata.reshape(argsA.batch_size,argsA.seq_length, -1), num_batches, 2)\n",
    "    y_batches = np.split(ydata.reshape(argsA.batch_size,argsA.seq_length, -1), num_batches, 2)\n",
    "    return x_batches,y_batches,num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "argsA = args()\n",
    "model = Model(argsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_batches,y_batches,num_batches = create_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    batchPointer = 0\n",
    "    for e in xrange(argsA.num_epochs):\n",
    "        sess.run(tf.assign(model.lr, argsA.learning_rate * (argsA.decay_rate ** e)))\n",
    "        batchPointer = 0\n",
    "        state = model.initial_state.eval()\n",
    "        for b in xrange(num_batches):\n",
    "            start = time.time()\n",
    "            x, y = x_batches[b],y_batches[b]\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state: state}\n",
    "            train_loss, state, _,myOut = sess.run([model.cost, model.final_state, model.train_op,model.logits], feed)\n",
    "            end = time.time()\n",
    "        print 'Loss at epoch',e,':',train_loss\n",
    "        if (e == argsA.num_epochs -1):\n",
    "            checkpoint_path = os.path.join(argsA.save_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path, global_step = e * num_batches)\n",
    "            print \"model saved to {}\".format(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sequences from RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model_generate = Model(argsA,True)\n",
    "\n",
    "x_batches,y_batches,num_batches = create_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames_to_generate = 1000\n",
    "noise_to_add = 0.1\n",
    "reset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(argsA.save_dir)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    state = model_generate.initial_state.eval()\n",
    "    x = x_batches[50]\n",
    "    xs = []\n",
    "    for i in range(frames_to_generate):\n",
    "        feed = {model_generate.input_data: x, model_generate.initial_state: state}\n",
    "        state,x1 = sess.run([model_generate.final_state,model_generate.logits], feed)\n",
    "        xs.append(x1[0])\n",
    "        x = x1.reshape([1,1,64]) + np.random.uniform(-noise_to_add,noise_to_add,[1,1,64])\n",
    "        if i % 100 == 0 and reset == True:\n",
    "            #state = model_generate.cell.zero_state(1, tf.float32).eval()\n",
    "            x = x_batches[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newXs = np.array(xs)\n",
    "with open('./data/new'+project_name+'_Z.p','w') as f:\n",
    "    cPickle.dump(newXs,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generating video from new latent sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=1e-3, \n",
    "                                 batch_size=200,load_model = True,checkpoint_folder='./vae_cp/'+project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/new'+project_name+'_Z.p','r') as f:\n",
    "    dataNew = cPickle.load(f)\n",
    "    \n",
    "allFrames = []\n",
    "for i in range((len(dataNew)/vae.batch_size)):\n",
    "    newX = vae.generate(dataNew[i*vae.batch_size:(i+1)*vae.batch_size])\n",
    "    allFrames.append(newX)\n",
    "allFrames = np.vstack(np.array(allFrames))\n",
    "allFrames = np.reshape(allFrames,[len(allFrames),128,128,3])\n",
    "\n",
    "for i in range(len(allFrames)):\n",
    "    im = Image.fromarray((allFrames[i] * 256).astype('uint8'))\n",
    "    im.save('./outputs/'+project_name+'/frame'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we combine all the newly generated frames into a video again!\n",
    "\n",
    "`ffmpeg -framerate 10 -i frame%01d.png -c:v libx264 -r 30 -pix_fmt yuv420p out.mp4`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
